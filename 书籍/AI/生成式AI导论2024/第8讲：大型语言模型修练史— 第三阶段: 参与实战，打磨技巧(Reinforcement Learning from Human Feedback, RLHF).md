# 第8讲：大型语言模型修练史— 第三阶段: 参与实战，打磨技巧(Reinforcement Learning from Human Feedback, RLHF)

### 第三阶段训练：基于人类反馈的强化学习（RLHF）

* 大型语言模型（LLM）的第三阶段训练强调与用户的实时交互，让模型通过实际操作来精炼技能。
* RLHF 依赖用户反馈，用户会在模型生成的多个回答中选择更优的答案，从而使模型根据这些偏好调整输出。
* 这一阶段与前几阶段不同，前阶段的训练数据主要是无直接人类输入生成的，重点是从数据中学习，而 RLHF 则引入了实际用户互动的反馈机制。

---

### 各阶段训练的区别

* **第一阶段（预训练）**：主要依靠自监督学习，从海量数据中学习模式，无需人工指导。
* **第二阶段（指令微调）**：依赖人类生成的问题和答案，人工工作量较大，需要精心设计交互。
* **RLHF 阶段**：人类不再直接提供标准答案，而是评估两条模型生成的回答哪个更好，从而简化反馈过程。

---

### 强化学习机制

* 在 RLHF 中，模型根据用户的评价调整参数，学习生成更可能被认可的回答。
* 调整方式包括：增加被偏好答案的生成概率，降低不被偏好答案的生成概率。
* 该强化学习过程促使模型关注整体输出质量，而不仅仅是单个词或符号的正确性。

---

### 用户参与与反馈效率

* 与早期训练阶段需要用户生成正确答案不同，RLHF 只需用户在两个回答间选择更优者，降低了操作难度。
* 用户只需判断哪个回答更好，而无需自行构造标准答案，从而减轻了认知负担。
* 由于人力资源有限，高效收集用户反馈对 RLHF 成功至关重要。

---

### 定义“好回答”的挑战

* “好回答”的标准具有主观性，并会因上下文不同而变化，使反馈过程复杂化。
* 安全性与有用性可能会出现冲突，同一答案在不同评价指标下可能得到不同评分，增加模型训练的难度。
* 多奖励模型的平衡（如安全性 vs 有用性）需要仔细考虑，否则可能导致不一致的训练结果。

---

### 未来方向：AI驱动反馈

* 随着大型语言模型的进步，有可能使用 AI 对其他 AI 的输出进行评估，形成自我改进的循环。
* 从人类反馈转向 AI 驱动的评估，可提升训练效率，但同时也带来反馈质量与可靠性的问题。
* 挑战在于确保 AI 生成的反馈仍然符合人类价值观和偏好，尤其是在模型越来越复杂的情况下。
